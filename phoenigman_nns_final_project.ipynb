{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CMSC 25500 Final Project: Pierce Hoenigman\n",
        "\n",
        "Evaluation of Out-of-Bounds Pattern Conservation in Implicit Neural Representation Networks"
      ],
      "metadata": {
        "id": "NWQqXCc29uBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG-rJNxJIUqR"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "import torchaudio\n",
        "\n",
        "from PIL import Image\n",
        "import skimage\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flexible implicit representation network\n",
        "#Sitzmann et al.: https://arxiv.org/pdf/2006.09661, github: https://github.com/vsitzmann/siren/tree/master\n",
        "#Tancik et al.: https://arxiv.org/pdf/2006.10739, github: https://github.com/tancik/fourier-feature-networks/blob/master/Demo.ipynb\n",
        "#Mehrabian et al.: https://arxiv.org/pdf/2409.09323, github: https://github.com/Ali-Meh619/FKAN/blob/main/FKAN_INR.ipynb\n",
        "\n",
        "\n",
        "class Sinusoidal(nn.Module):\n",
        "  #inspired by Sitzmann\n",
        "  def __init__(self, input_dim, output_dim, freq=30, freq_first=False):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.omega = freq\n",
        "    self.omega_first = freq_first\n",
        "\n",
        "    self.layer = nn.Linear(input_dim, output_dim, bias=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if self.omega_first:\n",
        "          self.layer.weight.uniform_(-1/self.input_dim, 1/self.output_dim)\n",
        "      else:\n",
        "          self.layer.weight.uniform_(-np.sqrt(6/self.input_dim) / self.omega,\n",
        "                                      np.sqrt(6/self.input_dim) / self.omega)\n",
        "  def forward(self, x):\n",
        "    return torch.sin(self.omega * self.layer(x))\n",
        "\n",
        "class Radial(nn.Module):\n",
        "  #inspired by Sitzmann\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.centers = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
        "        nn.init.uniform_(self.centers, -1, 1) #bounds between -1 and 1\n",
        "        self.sigs = nn.Parameter(torch.Tensor(output_dim))\n",
        "        nn.init.constant_(self.sigs, 10)\n",
        "\n",
        "  def forward(self, input):\n",
        "      input = input[0, ...]\n",
        "      size = (input.size(0), self.output_dim, self.input_dim)\n",
        "      x = input.unsqueeze(1).expand(size)\n",
        "      c = self.centers.unsqueeze(0).expand(size)\n",
        "      distances = (x - c).pow(2).sum(-1) * self.sigs.unsqueeze(0)\n",
        "      return self.gaussian(distances).unsqueeze(0)\n",
        "\n",
        "  def gaussian(self, x):\n",
        "      return torch.exp(-1 * x**2)\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "  #inspired by Mehrabian\n",
        "  #using Chebyshev polynomials rather than splines\n",
        "  def __init__(self, input_dim, output_dim, gridsize):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.gridsize = gridsize\n",
        "\n",
        "    self.coeffs = torch.nn.Parameter(torch.randn(2, output_dim, input_dim, gridsize) /\n",
        "                                            (np.sqrt(input_dim) * np.sqrt(gridsize)))\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(1, output_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    k = torch.reshape(torch.arange(1, self.gridsize+1, device=x.device), (1,1,1,self.gridsize))\n",
        "    x = torch.reshape(x, (x.shape[1],1,x.shape[2],1))\n",
        "\n",
        "    out =  torch.sum(torch.cos(k*x) * self.coeffs[0:1],(-2,-1))\n",
        "    out += torch.sum(torch.sin(k*x) * self.coeffs[1:2],(-2,-1))\n",
        "    out += self.bias\n",
        "    out = torch.reshape(out, (-1, self.output_dim))\n",
        "    return out\n",
        "\n",
        "class Flex_Model(nn.Module):\n",
        "  #inspired by Sitzmann\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim=256, nhidden=3, freq=30, activation='sin'):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.nhidden = nhidden\n",
        "    self.freq = freq\n",
        "    self.activation = activation\n",
        "\n",
        "    if activation=='sin':\n",
        "      self.modellst = nhidden * [Sinusoidal(hidden_dim, hidden_dim, freq=freq, freq_first=False)]\n",
        "      self.modellst.insert(0, Sinusoidal(input_dim, hidden_dim, freq=freq, freq_first=True))\n",
        "      finallayer = nn.Linear(hidden_dim, output_dim)\n",
        "      with torch.no_grad():\n",
        "        finallayer.weight.uniform_(-np.sqrt(6 / hidden_dim) / freq, np.sqrt(6 / hidden_dim) / freq)\n",
        "      self.modellst.append(finallayer)\n",
        "      #self.modellst.append(Sinusoidal(hidden_dim, output_dim, freq=freq, freq_first=False))\n",
        "\n",
        "    elif activation in ['relu', 'ffn']:\n",
        "      if activation == 'ffn':\n",
        "        #inspired by Tancik\n",
        "        assert input_dim % 2 == 0, 'for ffn, input dim needs to be even'\n",
        "        self.B = torch.randn((input_dim // 2,2)).to(device) * 10\n",
        "      self.modellst = nhidden * [nn.Linear(hidden_dim, hidden_dim, bias=True), nn.ReLU()]\n",
        "      self.modellst.insert(0, nn.ReLU())\n",
        "      self.modellst.insert(0, nn.Linear(input_dim, hidden_dim, bias=True))\n",
        "      self.modellst.append(nn.Linear(hidden_dim, output_dim, bias=True))\n",
        "\n",
        "    elif activation=='tanh':\n",
        "      self.modellst = nhidden * [nn.Linear(hidden_dim, hidden_dim, bias=True), nn.Tanh()]\n",
        "      self.modellst.insert(0, nn.Tanh())\n",
        "      self.modellst.insert(0, nn.Linear(input_dim, hidden_dim, bias=True))\n",
        "      self.modellst.append(nn.Linear(hidden_dim, output_dim, bias=True))\n",
        "\n",
        "    elif activation=='rbf':\n",
        "      self.modellst = nhidden * [nn.Linear(hidden_dim, hidden_dim, bias=True), nn.ReLU()]\n",
        "      #self.modellst.insert(0, nn.ReLU())\n",
        "      self.modellst.insert(0, Radial(input_dim, hidden_dim))\n",
        "      self.modellst.append(nn.Linear(hidden_dim, output_dim, bias=True))\n",
        "\n",
        "    elif activation=='fkan':\n",
        "      #inspired by Mehrabian\n",
        "      #the paper doesn't mention this but seems to use sin instead of ReLU; this seems like cheating\n",
        "      #so I will also check that this works--if not, that's an interesting result as well\n",
        "      self.gridsize = 100 #what they found worked well was 270\n",
        "      self.modellst = nhidden * [nn.Linear(hidden_dim, hidden_dim, bias=True), nn.ReLU()]\n",
        "      self.modellst.insert(0, nn.LayerNorm(hidden_dim))\n",
        "      self.modellst.insert(0, Fourier(input_dim, hidden_dim, self.gridsize))\n",
        "      self.modellst.append(nn.Linear(hidden_dim, output_dim, bias=True))\n",
        "\n",
        "    else:\n",
        "      raise ValueError('activation must be in [sin, relu, tanh, rbf, ffn, fkan]')\n",
        "\n",
        "    self.model = nn.Sequential(*self.modellst)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.clone().detach().requires_grad_(True)\n",
        "    if self.activation=='ffn':\n",
        "      #inspired by Tancik\n",
        "      x_proj = (2*np.pi*x) @ self.B.T\n",
        "      x = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], axis=-1)\n",
        "    output = self.model(x)\n",
        "    return output, x\n"
      ],
      "metadata": {
        "id": "f5MbRf8fIbVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preprocessing utils\n",
        "\n",
        "def get_img_tensor(fname, sidelength):\n",
        "  #inspired by Sitzmann\n",
        "  img = Image.open(fname)\n",
        "  transform = Compose([\n",
        "      Resize(sidelength),\n",
        "      ToTensor(),\n",
        "      Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "  ])\n",
        "  img = transform(img)\n",
        "  return img\n",
        "\n",
        "def get_mgrid(sidelen, dim=2):\n",
        "  #inspired by Sitzmann\n",
        "  tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "  mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "  mgrid = mgrid.reshape(-1, dim)\n",
        "  return mgrid\n",
        "\n",
        "class PatternFitting(Dataset):\n",
        "  #inspired by Sitzmann\n",
        "  def __init__(self, fname, sidelength, selected_channel=None):\n",
        "      super().__init__()\n",
        "      img = get_img_tensor(fname, sidelength)\n",
        "      #print(img.shape)\n",
        "      self.pixels = img.permute(1, 2, 0) #.view(-1, 1)\n",
        "      if selected_channel is not None:\n",
        "        self.pixels = self.pixels[:,:,selected_channel]\n",
        "        self.pixels = torch.reshape(self.pixels, (1,sidelength**2,1))\n",
        "        #print(self.pixels.shape)\n",
        "      self.coords = get_mgrid(sidelength, 2)\n",
        "\n",
        "  def __len__(self):\n",
        "      return 1\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      if idx > 0: raise IndexError\n",
        "      #print(self.coords.shape, self.pixels.shape)\n",
        "      return self.coords, self.pixels\n",
        "\n",
        "class AudioFitting(Dataset):\n",
        "  def __init__(self, aud_file, selected_channel=None):\n",
        "    self.aud_tensor, self.sample_rate = torchaudio.load(aud_file, format='mp3')\n",
        "    self.channels, self.len = self.aud_tensor.shape\n",
        "    self.aud_tensor = self.aud_tensor.reshape(1, self.len, self.channels)\n",
        "    #print(self.channels, self.len)\n",
        "    if selected_channel is not None:\n",
        "      self.aud_tensor = self.aud_tensor[:, :, selected_channel].unsqueeze(2)\n",
        "      #print(self.aud_tensor.shape)\n",
        "    self.coords = torch.linspace(-1, 1 , steps=self.len)\n",
        "\n",
        "  def get_time(self):\n",
        "    return self.len\n",
        "\n",
        "  def __len__(self):\n",
        "    return 1\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx > 0: raise IndexError\n",
        "\n",
        "    return self.coords, self.samples\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vMCKF69YIb8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image training utils\n",
        "\n",
        "def train_siren(img_siren, dataloader, total_steps, img_dim, steps_til_summary=10, lr=1e-4):\n",
        "  optim = torch.optim.Adam(lr=lr, params=img_siren.parameters())\n",
        "\n",
        "  model_input, ground_truth = next(iter(dataloader))\n",
        "  model_input, ground_truth = model_input.to(device), ground_truth.to(device)\n",
        "  ground_truth = torch.squeeze(ground_truth)\n",
        "\n",
        "  for step in range(total_steps):\n",
        "    model_output, coords = img_siren(model_input)\n",
        "    loss = F.mse_loss(torch.squeeze(model_output), ground_truth)\n",
        "\n",
        "    if not step % steps_til_summary:\n",
        "      near_cell = show_cell(img_siren, img_dim, xcell_idx=0, ycell_idx=1, magnif=1, just_return=True)\n",
        "      near_loss = F.mse_loss(torch.squeeze(near_cell), ground_truth)\n",
        "      far_cell = show_cell(img_siren, img_dim, xcell_idx=-10, ycell_idx=10, magnif=1, just_return=True)\n",
        "      far_loss = F.mse_loss(torch.squeeze(far_cell), ground_truth)\n",
        "      wide_area = show_cell(img_siren, img_dim, xcell_idx=0, ycell_idx=0, magnif=10, just_return=True)\n",
        "\n",
        "      print(f\"Step {step}, Total loss {loss:.6f}, Near loss {near_loss:.6f}, Far loss {far_loss:.6f}\")\n",
        "\n",
        "      fig, axes = plt.subplots(1,4, figsize=(16,4))\n",
        "      axes[0].imshow(model_output.cpu().view(img_dim,img_dim).detach().numpy())\n",
        "      axes[1].imshow(near_cell.cpu().cpu().view(img_dim,img_dim).detach().numpy())\n",
        "      axes[2].imshow(far_cell.cpu().view(img_dim,img_dim).detach().numpy())\n",
        "      axes[3].imshow(wide_area.cpu().view(img_dim,img_dim).detach().numpy())\n",
        "      plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "def show_cell(img_siren, img_dim=256, xcell_idx=0, ycell_idx=0, magnif=1, shift_factor = 1, just_return=False):\n",
        "  with torch.no_grad():\n",
        "    out_of_range_coords = get_mgrid(img_dim, 2) * magnif\n",
        "    out_of_range_coords[:,0] = out_of_range_coords[:,0] + int(xcell_idx * shift_factor * img_dim)\n",
        "    out_of_range_coords[:,1] = out_of_range_coords[:,1] + int(ycell_idx * shift_factor * img_dim)\n",
        "    if torch.cuda.is_available():\n",
        "      out_of_range_coords = out_of_range_coords.cuda()\n",
        "    out_of_range_coords = torch.unsqueeze(out_of_range_coords, 0)\n",
        "    model_out, _ = img_siren(out_of_range_coords)\n",
        "    if just_return:\n",
        "      return model_out\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16,16))\n",
        "    ax.imshow(model_out.cpu().view(1024,1024).numpy())\n",
        "    plt.show()\n",
        "\n",
        "def eval_square(img_siren, img_dim, ground_truth, near_dist=1, sf=1):\n",
        "  losses = []\n",
        "  for cell in range(-near_dist, near_dist+1):\n",
        "    c1 = show_cell(img_siren, img_dim, xcell_idx=-near_dist, ycell_idx=cell, magnif=1, shift_factor=sf, just_return=True)\n",
        "    c2 = show_cell(img_siren, img_dim, xcell_idx=near_dist, ycell_idx=cell, magnif=1, shift_factor=sf, just_return=True)\n",
        "    c3 = show_cell(img_siren, img_dim, xcell_idx=cell, ycell_idx=-near_dist, magnif=1, shift_factor=sf, just_return=True)\n",
        "    c4 = show_cell(img_siren, img_dim, xcell_idx=cell, ycell_idx=near_dist, magnif=1, shift_factor=sf, just_return=True)\n",
        "    losses = losses + [F.mse_loss(torch.squeeze(c1), ground_truth), F.mse_loss(torch.squeeze(c2), ground_truth),\n",
        "                       F.mse_loss(torch.squeeze(c3), ground_truth), F.mse_loss(torch.squeeze(c4), ground_truth)]\n",
        "  losses = torch.tensor(losses)\n",
        "  return (torch.mean(losses), torch.std(losses))\n",
        "\n",
        "def eval_model(img_siren, img_dim, ground_truth, modinfo, near_dist=1, far_dist=10, sf=1, verbose=True):\n",
        "  regular_loss, regular_sd = eval_square(img_siren, img_dim, ground_truth, near_dist=0, sf=sf)\n",
        "  near_loss, near_sd = eval_square(img_siren, img_dim, ground_truth, near_dist=near_dist, sf=sf)\n",
        "  far_loss, far_sd = eval_square(img_siren, img_dim, ground_truth, near_dist=far_dist, sf=sf)\n",
        "  if verbose:\n",
        "    print(f\"########## MODEL EVAL ##########\\nModel: {modinfo}\\nRegular loss: {regular_loss}, Dist {near_dist} tile loss: {near_loss},\\\n",
        "          Dist {far_dist} tile loss: {far_loss}\\nRegular SD: {regular_sd}, Dist {near_dist} tile SD: {near_sd},\\\n",
        "          Dist {far_dist} tile SD: {far_sd}\\n########## MODEL EVAL ##########\")\n",
        "  return [modinfo, regular_loss, near_loss, far_loss, regular_sd, near_sd, far_sd]\n",
        "\n"
      ],
      "metadata": {
        "id": "VT-DCOW3t0LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Audio training utils\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ro9g90l9t0Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data loading\n",
        "\n",
        "img_patterns = ['checker1.png','checker2.png','checker3.png','tartan1.png','tartan2.png','tartan3.png']\n",
        "aud_patterns = [] #['bartok1.mp3'] #['hihat1.mp3','hihat3.mp3','hihat2-5.mp3','bartok1.mp3','bartok2.mp3','bartok1-5.mp3']\n",
        "sidelen = 128 #256\n",
        "#img_shift_factor = [1,1,1.2,1,1,1.2]\n",
        "#aud_shift_factor = [1,1,1.2,1,1,1.33]\n",
        "\n",
        "### Loading in IMAGES ###\n",
        "img_pattern_fits = []\n",
        "img_dataloaders = []\n",
        "for pattern_file in img_patterns:\n",
        "  img_pattern_fit = PatternFitting(pattern_file, sidelen, selected_channel=0)\n",
        "  img_dataloader = DataLoader(img_pattern_fit, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "  img_pattern_fits.append(img_pattern_fit)\n",
        "  img_dataloaders.append(img_dataloader)\n",
        "\n",
        "fkan_pattern_fits = []\n",
        "fkan_dataloaders = []\n",
        "for pattern_file in img_patterns:\n",
        "  fkan_pattern_fit = PatternFitting(pattern_file, sidelen // 2, selected_channel=0)\n",
        "  fkan_dataloader = DataLoader(fkan_pattern_fit, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "  fkan_pattern_fits.append(fkan_pattern_fit)\n",
        "  fkan_dataloaders.append(fkan_dataloader)\n",
        "\n",
        "### Loading in AUDIO ###\n",
        "aud_fits = []\n",
        "aud_dataloaders = []\n",
        "for aud_file in aud_patterns:\n",
        "  aud_fitting = AudioFitting(aud_file, selected_channel=0)\n",
        "  aud_dataloader = DataLoader(aud_fitting, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "  aud_fits.append(aud_fitting)\n",
        "  aud_dataloaders.append(aud_dataloader)\n"
      ],
      "metadata": {
        "id": "iWTNM96jt0rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image training and eval loop\n",
        "model_types = ['relu', 'tanh', 'rbf', 'sin', 'ffn', 'fkan']\n",
        "\n",
        "final_losses = []\n",
        "models = []\n",
        "for mtype in model_types:\n",
        "  for i, img in enumerate(img_dataloaders):\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'########## Starting to train model {mtype} on data {img_patterns[i]} ##########')\n",
        "    indim = 256 if mtype=='ffn' else 2\n",
        "    model = Flex_Model(indim, 1, hidden_dim=256, nhidden=3, freq=30, activation=mtype).to(device)\n",
        "\n",
        "    img = fkan_dataloaders[i] if mtype=='fkan' else img\n",
        "    sidelen = 64 if mtype=='fkan' else 128\n",
        "    nsteps = 301 if i < 3 else 601\n",
        "    train_siren(model, img, total_steps=nsteps, img_dim=sidelen, steps_til_summary=(nsteps - 1) // 3, lr=1e-4) #provides visuals\n",
        "\n",
        "    models.append((f'{mtype} / {img_patterns[i]}', model.to('cpu')))\n",
        "    #_, ground_truth = next(iter(img))\n",
        "    #ground_truth = ground_truth.to(device)\n",
        "    #losses = eval_model(model, sidelen, ground_truth, f'{mtype} / {img_patterns[i]}', near_dist=1, far_dist=2, verbose=True)\n",
        "    #final_losses.append(losses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LMIHBEX_wX42",
        "outputId": "c66ec40f-e461-41b1-86fd-e534b2e10d95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mtype in ['ffn']:\n",
        "  for i, img in enumerate(img_dataloaders):\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'########## Starting to train model {mtype} on data {img_patterns[i]} ##########')\n",
        "    indim = 256 if mtype=='ffn' else 2\n",
        "    model = Flex_Model(indim, 1, hidden_dim=256, nhidden=3, freq=30, activation=mtype).to(device)\n",
        "\n",
        "    img = fkan_dataloaders[i] if mtype=='fkan' else img\n",
        "    sidelen = 64 if mtype=='fkan' else 128\n",
        "    nsteps = 301 if i < 3 else 601\n",
        "    train_siren(model, img, total_steps=nsteps, img_dim=sidelen, steps_til_summary=(nsteps - 1) // 3, lr=1e-4) #provides visuals\n",
        "\n",
        "    models.append((f'{mtype} / {img_patterns[i]}', model.to('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OrK5GN7wY1UQ",
        "outputId": "41e63ba7-8fe3-4ab0-8290-af1b8ee07d53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mtype in ['fkan']:\n",
        "  for i, img in enumerate(img_dataloaders):\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'########## Starting to train model {mtype} on data {img_patterns[i]} ##########')\n",
        "    indim = 256 if mtype=='ffn' else 2\n",
        "    model = Flex_Model(indim, 1, hidden_dim=256, nhidden=3, freq=30, activation=mtype).to(device)\n",
        "\n",
        "    img = fkan_dataloaders[i] if mtype=='fkan' else img\n",
        "    sidelen = 64 if mtype=='fkan' else 128\n",
        "    nsteps = 301 if i < 3 else 601\n",
        "    train_siren(model, img, total_steps=nsteps, img_dim=sidelen, steps_til_summary=(nsteps - 1) // 3, lr=1e-4) #provides visuals\n",
        "\n",
        "    models.append((f'{mtype} / {img_patterns[i]}', model.to('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x8K6HP-1eJgD",
        "outputId": "f314aa91-ae39-4868-b826-c8b40b5904f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Audio training and eval loop"
      ],
      "metadata": {
        "id": "OVpmLj53wbfc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}